---
title: "variance-bias-tradeoff"
output: html_document
editor_options: 
  markdown: 
    wrap: 72
---

```{r}
# import data from the web
library(readr)
df <- read_table("http://azzalini.stat.unipd.it/Book-DM/yesterday.dat", 
                  col_types = cols( x = col_double(),
                                    y.yesterday = col_double(),
                                    y.tomorrow = col_double())
                  )[-31,]

# create training set
train <- data.frame(x = df$x, y = df$y.yesterday)
```

Now what is this mockery!!

```{r}
plot(y ~ x, train)
```

```{r}
# create test set
test = data.frame(x = train$x, y = df$y.tomorrow)

# 3rd degree polynomial regression fit
fit <- lm( y ~ poly(x, degree = 3), train)
yhat <- predict(fit, newdata = test)

# plot
plot(y ~ x , train)
lines(yhat ~ x, train)
```

```{r}
# compute MSE.tr for the 3rd degree polynomial fit
MSE.tr <- mean( (train$y - yhat)^2 )
MSE.tr
```

```{r}
n  <- nrow(train)
ds <- 0:(n-1)
ps <- ds + 1
# function to fit polynomial model of degree d
fun <- function(d) if (d == 0) lm(y ~ 1, train) else 
                   lm(y ~ poly(x, degree = d, raw = T), train)
fits <- sapply(ds, fun)       ### list of models

# compute MSE.tr for all degrees
MSEs.tr <- unlist( lapply(fits, deviance) )/n
plot(ds, MSEs.tr, type = "b", xlab = "d", ylab = "MSE.tr",
     col = "darkslategray", lwd=2)
```

```{r}
# 20th degree polynomial fit
fit <- lm( y ~ poly(x, degree = 20), train)
yhat <- predict(fit, newdata = test)

# design matrix
X = model.matrix(fit)

# plots
plot(y ~ x, train)
lines(yhat ~ x, train)
plot(y ~ x, test, col = 4)
lines(yhat ~ x, train)
```

```{r}
# compute predictions for all degrees
yhats <- lapply(fits, predict)
# compute MSE.te for all degrees
MSEs.te <- unlist(lapply(yhats, 
           function(yhat) mean((test$y - yhat)^2))
           )
# plot
plot(ds, MSEs.te, type = "b", col = 4, xlab = "d", ylab = "MSE.te")
```

```{r}
# which d gives minimum MSE.te?
ds[which.min(MSEs.te)]
```

# Moving on to the Bias Variance Trade-off

```{r}
# true regression function 
ftrue <- c(0.4342, 0.4780, 0.5072, 0.5258, 0.5369, 0.5426, 0.5447,
           0.5444, 0.5425, 0.5397, 0.5364, 0.5329, 0.5294, 0.5260,
           0.5229, 0.5200, 0.5174, 0.5151, 0.5131, 0.5113, 0.5097,
           0.5083, 0.5071, 0.5061, 0.5052, 0.5044, 0.5037, 0.5032,
           0.5027, 0.5023)
x <- seq(.5, 3, length = 30)
plot(x, ftrue, type = "l", col = 4, main="The signal")
```

```{r}
d <- 5
sigmatrue <- 0.01

# design matrix
X <- model.matrix(lm(ftrue ~ poly(x, degree = d)))
invXtX <- solve(crossprod(X))

# Bias
Bias2 <- ( apply(X, 1, function(x) 
  x %*% invXtX %*% t(X) %*% ftrue) - ftrue)^2

# Variance
Var <- apply(X, 1, function(x) 
  sigmatrue^2 * t(x) %*% invXtX %*% x
)

barplot(Bias2 + Var, ylab = "Bias2 + Var", names.arg = round(x, 1), 
        col = "orange", ylim=c(0,0.00012), main="d=5")
barplot(Var, add = T, col = 4, names.arg=" ")
legend("topright", c("Bias2", "Var"), col=c("orange", 4), 
       pch = c(19, 19))
```






# Exercise

```{r}
set.seed(0)
n = 50
p = 30
X = matrix(rnorm(n*p),nrow=n)
```

```{r}
bstar = c(runif(10,0.5,1),runif(20,0,0.3)) # oracle
mu = as.numeric(X%*%bstar)

hist(bstar,breaks=30,col="gray",main="",
xlab="True coefficients")
```
